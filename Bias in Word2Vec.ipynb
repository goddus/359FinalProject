{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410e6857",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "Below we have cells dedicated to installing some of the necessary packages if you do not already have them. You only need to uncomment and run these cells once. You may need to restart your kernel, but once done you can recomment or delete the cells entirely. PRO TIP: You can comment/uncomment a cell easily in jupyter notebook via CTRL+'/' or CMD+'/'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bee4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a71fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f450d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f7348",
   "metadata": {},
   "source": [
    "# Setup\n",
    "First we import our necessary packages. \n",
    "\n",
    "The cell after will load in a model - for now don't worry too much about this model but the cell will probably take a while to run, it is loading in a model that is quite large! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306d1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948dbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "660d224f",
   "metadata": {},
   "source": [
    "# Section 1 - Introduction to NLP and Word2Vec\n",
    "Natural Language Processing is a branch of AI that focuses on getting machines to understand language as spoken by humans. The image below shows an example of a simple way to process language called document summarization. It takes a corpus of documents and calculates the frequency of different topics, based on the words that appear in the corpus. This is an easy way to determine general ideas presented in a corpus. But how do we get a machine to learn from what they read?\n",
    "<div>\n",
    "    <img src=\"images/NLP.png\"/> \n",
    "</div>\n",
    "\n",
    "Word2Vec is an algorithm for natural language processing that is able to learn from a corpus and associate words with other words. It does this using a neural network to create word embeddings. Word embeddings are a numerical representation of a word, typically a vector of real numbers. If you’re interested in learning more about how Word2Vec works, visit the following links: https://jalammar.github.io/illustrated-word2vec/ https://www.tensorflow.org/tutorials/text/word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e4bd",
   "metadata": {},
   "source": [
    "# Section 2 - Using Word2Vec\n",
    "Let's start off simple. Below we use the model to get the word embedding of the word \"car\" and print out the vector representation. Woah, that's a lot! We can already start to see the mysterious black box that comes with a lot of machine learning tools that rely on neural networks. A whole lot of numbers - but but do we really know what they mean?\n",
    "\n",
    "**2.1 Code this:** Get the shape of the embedding and print it out. Also try a second word embedding on whatever word you would like and do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c946da94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812\n",
      "  0.04931641 -0.16894531  0.20898438  0.11962891  0.18066406 -0.25\n",
      " -0.10400391 -0.10742188 -0.01879883  0.05200195 -0.00216675  0.06445312\n",
      "  0.14453125 -0.04541016  0.16113281 -0.01611328 -0.03088379  0.08447266\n",
      "  0.16210938  0.04467773 -0.15527344  0.25390625  0.33984375  0.00756836\n",
      " -0.25585938 -0.01733398 -0.03295898  0.16308594 -0.12597656 -0.09912109\n",
      "  0.16503906  0.06884766 -0.18945312  0.02832031 -0.0534668  -0.03063965\n",
      "  0.11083984  0.24121094 -0.234375    0.12353516 -0.00294495  0.1484375\n",
      "  0.33203125  0.05249023 -0.20019531  0.37695312  0.12255859  0.11425781\n",
      " -0.17675781  0.10009766  0.0030365   0.26757812  0.20117188  0.03710938\n",
      "  0.11083984 -0.09814453 -0.3125      0.03515625  0.02832031  0.26171875\n",
      " -0.08642578 -0.02258301 -0.05834961 -0.00787354  0.11767578 -0.04296875\n",
      " -0.17285156  0.04394531 -0.23046875  0.1640625  -0.11474609 -0.06030273\n",
      "  0.01196289 -0.24707031  0.32617188 -0.04492188 -0.11425781  0.22851562\n",
      " -0.01647949 -0.15039062 -0.13183594  0.12597656 -0.17480469  0.02209473\n",
      " -0.1015625   0.00817871  0.10791016 -0.24609375 -0.109375   -0.09375\n",
      " -0.01623535 -0.20214844  0.23144531 -0.05444336 -0.05541992 -0.20898438\n",
      "  0.26757812  0.27929688  0.17089844 -0.17578125 -0.02770996 -0.20410156\n",
      "  0.02392578  0.03125    -0.25390625 -0.125      -0.05493164 -0.17382812\n",
      "  0.28515625 -0.23242188  0.0234375  -0.20117188 -0.13476562  0.26367188\n",
      "  0.00769043  0.20507812 -0.01708984 -0.12988281  0.04711914  0.22070312\n",
      "  0.02099609 -0.29101562 -0.02893066  0.17285156  0.04272461 -0.19824219\n",
      " -0.04003906 -0.16992188  0.10058594 -0.09326172  0.15820312 -0.16503906\n",
      " -0.06054688  0.19433594 -0.07080078 -0.06884766 -0.09619141 -0.07226562\n",
      "  0.04882812  0.07324219  0.11035156  0.04858398 -0.17675781 -0.33789062\n",
      "  0.22558594  0.16308594  0.05102539 -0.08251953  0.07958984  0.08740234\n",
      " -0.16894531 -0.02160645 -0.19238281  0.03857422 -0.05102539  0.21972656\n",
      "  0.08007812 -0.21191406 -0.07519531 -0.15039062  0.3046875  -0.17089844\n",
      "  0.12353516 -0.234375   -0.10742188 -0.06787109  0.01904297 -0.14160156\n",
      " -0.22753906 -0.16308594  0.14453125 -0.15136719 -0.296875    0.22363281\n",
      " -0.10205078 -0.0456543  -0.21679688 -0.09033203  0.09375    -0.15332031\n",
      " -0.01550293  0.3046875  -0.23730469  0.08935547  0.03710938  0.02941895\n",
      " -0.28515625  0.15820312 -0.00306702  0.06054688  0.00497437 -0.15234375\n",
      " -0.00836182  0.02197266 -0.12109375 -0.13867188 -0.2734375  -0.06835938\n",
      "  0.08251953 -0.26367188 -0.16992188  0.14746094  0.08496094  0.02075195\n",
      "  0.13671875 -0.04931641 -0.0100708  -0.00369263 -0.10839844  0.14746094\n",
      " -0.15527344  0.16113281  0.05615234 -0.05004883 -0.1640625  -0.26953125\n",
      "  0.4140625   0.06079102 -0.046875   -0.02514648  0.10595703  0.1328125\n",
      " -0.16699219 -0.04907227  0.04663086  0.05151367 -0.07958984 -0.16503906\n",
      " -0.29882812  0.06054688 -0.15332031 -0.00598145  0.06640625 -0.04516602\n",
      "  0.24316406 -0.07080078 -0.36914062 -0.23144531 -0.11914062 -0.08300781\n",
      "  0.14746094 -0.05761719  0.23535156 -0.12304688  0.14648438  0.13671875\n",
      "  0.15429688  0.02111816 -0.09570312  0.05859375  0.03979492 -0.08105469\n",
      "  0.0559082  -0.16601562  0.27148438 -0.20117188 -0.00915527  0.07324219\n",
      "  0.10449219  0.34570312 -0.26367188  0.02099609 -0.40039062 -0.03417969\n",
      " -0.15917969 -0.08789062  0.08203125  0.23339844  0.0213623  -0.11328125\n",
      "  0.05249023 -0.10449219 -0.02380371 -0.08349609 -0.04003906  0.01916504\n",
      " -0.01226807 -0.18261719 -0.06787109 -0.08496094 -0.03039551 -0.05395508\n",
      "  0.04248047  0.12792969 -0.27539062  0.28515625 -0.04736328  0.06494141\n",
      " -0.11230469 -0.02575684 -0.04125977  0.22851562 -0.14941406 -0.15039062]\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding for the word \"car\"\n",
    "word = \"car\"\n",
    "word_embedding = model[word]\n",
    "print(word_embedding)\n",
    "\n",
    "# CODE THIS:\n",
    "embedding_shape = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3fbc7",
   "metadata": {},
   "source": [
    "**2.2 Answer this:** Are the shapes of the two embeddings the same? \n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0206b",
   "metadata": {},
   "source": [
    "Now, we explore the \"power\" of Word2Vec. The way gensim implements Word2Vec gives us a couple of very useful functions. We explore *most_similar* and *similarity* below. Also note: these functions can take some time, especially since the model we are using is huge. For example, *most_similar* needs to find the word embedding for the word we choose and compare it to all others to find the most similar ones. Similarity takes values between (0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f1ca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vehicle', 0.7821096181869507),\n",
       " ('cars', 0.7423831224441528),\n",
       " ('SUV', 0.7160962820053101),\n",
       " ('minivan', 0.6907036304473877),\n",
       " ('truck', 0.6735789775848389),\n",
       " ('Car', 0.667760968208313),\n",
       " ('Ford_Focus', 0.6673202514648438),\n",
       " ('Honda_Civic', 0.662684977054596),\n",
       " ('Jeep', 0.6511331796646118),\n",
       " ('pickup_truck', 0.64414381980896)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3f058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(word,\"van\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174de1f",
   "metadata": {},
   "source": [
    "Similarity between word embeddings simply means some sort of mathematical metric (usually cosine similarity) of similarity between the vectors themselves. So while we are attempting to capture some sense of \"similarity\" between two words, this measure does not always transfer over to our ideas of what words are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1d5b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24105665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(word,\"electric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77145ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3779698"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(word,\"plane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748ba88",
   "metadata": {},
   "source": [
    "**2.3 Answer this:** What do you personally think is the similarity between \"car\" and \"electric\" as well as \"car\" and \"plane\" and which which you consider \"more similar\"? How do you think Word2Vec is doing in capturing these similarities in this case, and what worries or excites you about this?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c025e99",
   "metadata": {},
   "source": [
    "What excited so many computer scientists when Word2Vec was first created was not only the ability to capture similarity between words, but actually use those similarities to encompass some critical ideas. As a human, you can probably finish this sentence for me: \"Berlin is to Germany, as ___ is to France\". What's actually quite cool, is that Word2Vec can take advantage of this, and this is one of the more famous examples in the a lot of literature (you will see this example referenced a lot if you've done any work with Word2Vec): Berlin - Germany + France. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ae5f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 0.7672388553619385),\n",
       " ('French', 0.6049168109893799),\n",
       " ('Parisian', 0.5810437202453613),\n",
       " ('Colombes', 0.5599985122680664),\n",
       " ('Hopital_Europeen_Georges_Pompidou', 0.555890679359436),\n",
       " ('Melun', 0.5512701272964478),\n",
       " ('Dinard', 0.5451847910881042),\n",
       " ('Brussels', 0.5420990586280823),\n",
       " ('Mairie_de', 0.5337448120117188),\n",
       " ('Cagnes_sur_Mer', 0.531246542930603)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"France\",\"Berlin\"],negative=[\"Germany\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df57eb5",
   "metadata": {},
   "source": [
    "**2.4 Code this:** Try this out for another pairing of capital-countries. Or maybe think of another application entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b19acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE THIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6d9a2",
   "metadata": {},
   "source": [
    "Word2Vec can pretty accurately encompass the idea of a capital. Though we do not have it coded, the same can be said about verb tense (see the figure in the next section for an example) and other ideas. Additionally, the idea of using NLP for predictions is everywhere. You use it almost everyday when you search and you have a bunch of suggestions of how to finish your search query. In fact, this one of the major reasons NLP was researched in the first place. \n",
    "\n",
    "**2.5 Answer this:** Where else have you seen NLP used? What worries or excites you about prediction or other applications you've seen?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7155857d",
   "metadata": {},
   "source": [
    "# Section 3 - Starting to investigate bias in Word2Vec\n",
    "As we’ve discussed a lot in this course, AI and machine learning models are not perfect. And with imperfection comes bias. In this section, we’ll explore different word embeddings that imply bias. \n",
    "\n",
    "As we explored briefly in the previous section, similarity is an abstract concept. What we think of when we say similarity does not entirely match the underlying mathematical similarity we compute. Word2Vec does not look at the context of the words on which it was trained on. Instead, it simply looks at associations between how often other words show up near the word we care about. For a more in depth look into how Word2Vec operates see this link about continuous bag of words (CBOW) and skip-gram: https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "\n",
    "<!-- <table><tr>\n",
    "<td> <img src=\"images/verb_tense.png\" alt=\"Verb Tense\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"images/king_queen.png\" alt=\"King Queen\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table> -->\n",
    "<div>\n",
    "    <img src=\"images/verb_tense.png\" width=\"250\"/>   \n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71539d82",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"images/king_queen.png\" width=\"250\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071b980",
   "metadata": {},
   "source": [
    "## Be careful! \n",
    "It's easy to assume bias and falsely attribute some causal relationship to the data. Like the saying goes, \"if you go looking for something, you'll find it\". As with this entire assignment, we don't actually know why we are seeing the number we are seeing - we have might have an idea or some worries, but we do not ultimately know. Is there a racial bias present amongst articles which discuss painters? Well, maybe, but we should not assume so based solely off what we present in this assignment. Word2Vec ignores context, so although black and white might be loosely associated with painter it is hard to tell if its because of race or simply the colors black and white with which painters work with all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac5e513b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10742523\n",
      "0.14594205\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity(\"painter\",\"black\"))\n",
    "print(model.similarity(\"painter\",\"white\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e1b2a",
   "metadata": {},
   "source": [
    "As promised, Word2Vec also can be used to relate verb tenses. \n",
    "\n",
    "**3.1 Code This:** Use the figure above and some of your code from 2.4 to use walking, walked, and one tense of swim to get the other tense of swim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0c1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE THIS\n",
    "# model.most_similar(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273b0cc",
   "metadata": {},
   "source": [
    "We have also finally arrived at probably the most famous result from Word2Vec, the King Queen example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096ac54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"king\",\"woman\"],negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58febf47",
   "metadata": {},
   "source": [
    "**3.2 Answer this:** What human concept are we trying to exploit using Word2Vec? Why might this be a useful result? What are the dangers of entrusting a machine to interpret a human concept?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a1587b9",
   "metadata": {},
   "source": [
    "Delving deeper into areas of bias...\n",
    "<div>\n",
    "    <img src=\"images/occupations.jpg\" width=\"500\"/> \n",
    "</div>\n",
    "\n",
    "**3.3 Code this:** Visit this great visualization by Nathan Yau of the percentage of male/female workers in certain occupations plotted on a spectrum. Instead of king-queen, try out some male or female dominated occupations. We've done one of the most studied cases for you. NOTE: Be careful of the difference between male or female dominated fields. Would you try to do \n",
    "\n",
    "https://flowingdata.com/2017/09/11/most-female-and-male-occupations-since-1950/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae69dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homemaker', 0.5627118945121765),\n",
       " ('housewife', 0.5105046033859253),\n",
       " ('graphic_designer', 0.5051802396774292),\n",
       " ('schoolteacher', 0.497949481010437),\n",
       " ('businesswoman', 0.49348917603492737),\n",
       " ('paralegal', 0.49255111813545227),\n",
       " ('registered_nurse', 0.4907974600791931),\n",
       " ('saleswoman', 0.4881627559661865),\n",
       " ('electrical_engineer', 0.4797726571559906),\n",
       " ('mechanical_engineer', 0.4755399525165558)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"woman\",\"computer_programmer\"],negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d88c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d212f38",
   "metadata": {},
   "source": [
    "**3.4 Answer this:** What did you find? Were you surprised by any results in particular? Where do you think the potential problems lie?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c687c",
   "metadata": {},
   "source": [
    "# Section 4 - A different training corpus\n",
    "\n",
    "Up until now, we've been using one of the premier, gigantic corpuses made from Google News. A corpus is just a fancy way of saying a training set of sentences for Word2Vec to be trained on. As the name implies, the corpus is made from millions of articles available at the time of creation. \n",
    "\n",
    "A full list of models supported by gensim by default can be found here: https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py, but keep in mind you can also train Word2Vec on your own corpus! And many people have done so and published there models elsewhere.\n",
    "\n",
    "In this section we will explore the potential differences (and dangers) of using different corpuses of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6867cb",
   "metadata": {},
   "source": [
    "**4.1 Answer this:** Off the top of your head, what differences do you anticipate between using different corpuses? Do you suspect any overaching themes between using Google News versus Wikipedia or Twitter as our training grounds?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d9dc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will take a long time, but I promise it's cool! \n",
    "wiki_model = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "twitter_model = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda178b",
   "metadata": {},
   "source": [
    "We're ESE/CSE's after all, so we deal with circuits a lot. Let's see what each model thinks is most similar to a circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd74ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('circuits', 0.7530251741409302),\n",
       " ('circut', 0.6202580332756042),\n",
       " ('Pudenz_®_brand', 0.5677723288536072),\n",
       " ('Brands_Hatch_Grand_Prix', 0.5560801029205322),\n",
       " ('Brands_Hatch_GP', 0.5559306144714355),\n",
       " ('packet_narrowband', 0.5514232516288757),\n",
       " ('director_Ramon_Pradera', 0.5479720234870911),\n",
       " ('Valencia_Ricardo_Tormo', 0.5345189571380615),\n",
       " ('laps_###.###km', 0.5329551100730896),\n",
       " ('twisty_Ricardo_Tormo', 0.5329512357711792)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f38deeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('circuits', 0.5813344120979309),\n",
       " ('appeals', 0.5144815444946289),\n",
       " ('court', 0.46676933765411377),\n",
       " ('judge', 0.45699357986450195),\n",
       " ('appellate', 0.43504196405410767),\n",
       " ('courts', 0.42194798588752747),\n",
       " ('prix', 0.40702903270721436),\n",
       " ('suzuka', 0.39734673500061035),\n",
       " ('judges', 0.3888646960258484),\n",
       " ('9th', 0.38861793279647827)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\"circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5aa6771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('race', 0.5097535252571106),\n",
       " ('interval', 0.5071332454681396),\n",
       " ('grid', 0.495891809463501),\n",
       " ('cycle', 0.4931078851222992),\n",
       " ('motion', 0.4904831647872925),\n",
       " ('circuits', 0.4886697232723236),\n",
       " ('board', 0.47897377610206604),\n",
       " ('training', 0.47691550850868225),\n",
       " ('session', 0.4747787415981293),\n",
       " ('cardio', 0.4686027467250824)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_model.most_similar(\"circuit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58669468",
   "metadata": {},
   "source": [
    "You might see some very interesting results, take some time and answer the following.\n",
    "\n",
    "**4.2 Answer this:** What are the major themes across each model? Is this a useful thing? \n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e40a3",
   "metadata": {},
   "source": [
    "TRIGGER WARNING: Use of terror/terrorism and potential associations. We are trying to investigate potential bias, but we realize this can be harmful to some people. Skip this part if necessary.\n",
    "\n",
    "Next, we see what the different corpuses think of \"terrorist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0608c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terror', 0.8476202487945557),\n",
       " ('terrorists', 0.80003821849823),\n",
       " ('terrorism', 0.7522496581077576),\n",
       " ('al_Qaeda', 0.7314331531524658),\n",
       " ('Terrorist', 0.7178781032562256),\n",
       " ('Al_Qaeda', 0.7080404162406921),\n",
       " ('extremist', 0.681232750415802),\n",
       " (\"al_Qa'ida\", 0.6795461177825928),\n",
       " ('jihadist', 0.6788939237594604),\n",
       " ('al_Qaida', 0.6745679378509521)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"terrorist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38ef013c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrorists', 0.7696176767349243),\n",
       " ('terrorism', 0.7433343529701233),\n",
       " ('attacks', 0.6373909711837769),\n",
       " ('bombing', 0.6256234645843506),\n",
       " ('extremist', 0.6239424347877502),\n",
       " ('taliban', 0.6112457513809204),\n",
       " ('muslim', 0.5831813812255859),\n",
       " ('jihadist', 0.5722413659095764),\n",
       " ('communist', 0.5699522495269775),\n",
       " ('qaeda', 0.5643658638000488)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_model.most_similar(\"terrorist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa3b2a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terror', 0.8342181444168091),\n",
       " ('terrorists', 0.7970923781394958),\n",
       " ('terrorism', 0.781484842300415),\n",
       " ('attacks', 0.7183513641357422),\n",
       " ('qaida', 0.7041255235671997),\n",
       " ('qaeda', 0.6885311007499695),\n",
       " ('extremist', 0.6289576888084412),\n",
       " ('extremists', 0.6135696172714233),\n",
       " ('bombings', 0.6122392416000366),\n",
       " ('militant', 0.5995929837226868)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model.most_similar(\"terrorist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27471fe7",
   "metadata": {},
   "source": [
    "**4.3 Answer this:** Again, do you observe any themes across the different models? Is this a fair expectation, or is this a worrisome outcome?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e3ea64",
   "metadata": {},
   "source": [
    "Feel free to try other words or corpuses. You may find surprising thigns (or the lack thereof)! We just scratched the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a0396",
   "metadata": {},
   "source": [
    "# Section 5 - Exploring Names with Region & Ethnicity\n",
    "In this section we take a step back from direct bias and explore another way Word2Vec can be used. Here we take a look at using Word2Vec to predict where someone lives or what their ethnicity is. We decided to try to remove our own biases and simply use this wikipedia article as a reference for a list of names and regions: https://en.wikipedia.org/wiki/List_of_most_popular_given_names\n",
    "\n",
    "Below we have provided a function that does some sort of prediction (while also providing the values the model returns). We picked arbitrary names from the wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7a4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_ethnicity_name(name,regions,ethnicities):\n",
    "    most_sim_region = regions[0]\n",
    "    most_sim_ethnicity = ethnicities[0]\n",
    "    for i in range(len(regions)):\n",
    "        print(\"Region:\",regions[i],\"| Similarity to\",name,\":\",model.similarity(name,regions[i]))\n",
    "        print(\"Ethnicity:\",ethnicities[i],\"| Similarity to\",name,\":\",model.similarity(name,ethnicities[i]),\"\\n\")\n",
    "        if model.similarity(name,regions[i]) > model.similarity(name,most_sim_region):\n",
    "            most_sim_region = regions[i]\n",
    "        if model.similarity(name,ethnicities[i]) > model.similarity(name,most_sim_ethnicity):\n",
    "            most_sim_ethnicity = ethnicities[i]\n",
    "\n",
    "    print(\"Most similiar region to\",name,\":\",most_sim_region)\n",
    "    print(\"Most similiar ethnicity to\",name,\":\",most_sim_ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72b9b8",
   "metadata": {},
   "source": [
    "In this first example, we chose an easy case for us: the first name from the first region, Omar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "daabb9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: Africa | Similarity to Omar : 0.06157282\n",
      "Ethnicity: African | Similarity to Omar : 0.16177538 \n",
      "\n",
      "Region: Americas | Similarity to Omar : 0.049406413\n",
      "Ethnicity: American | Similarity to Omar : 0.1080503 \n",
      "\n",
      "Region: Asia | Similarity to Omar : -0.013686761\n",
      "Ethnicity: Asian | Similarity to Omar : 0.053079262 \n",
      "\n",
      "Region: Europe | Similarity to Omar : -0.08183579\n",
      "Ethnicity: European | Similarity to Omar : -0.043689683 \n",
      "\n",
      "Region: Oceania | Similarity to Omar : 0.002717767\n",
      "Ethnicity: Oceanic | Similarity to Omar : -0.016847825 \n",
      "\n",
      "Most similiar region to Omar : Africa\n",
      "Most similiar ethnicity to Omar : African\n"
     ]
    }
   ],
   "source": [
    "regions = [\"Africa\", \"Americas\",\"Asia\",\"Europe\",\"Oceania\"]\n",
    "ethnicities = [\"African\",\"American\",\"Asian\",\"European\",\"Oceanic\"]\n",
    "region_ethnicity_name(\"Omar\",regions,ethnicities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cabb0",
   "metadata": {},
   "source": [
    "In this next example, we chose an interesting name to highlight why this is practically a terrible way of predicting someone's location/ethnicity. Olivia actually occurs 17 times in the wikipedia article and our own bias would lead us to believe it would definitely be in the Americas. In reality, Olivia is of course just a common name across multiple regions and ethnicities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "848fa456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: Africa | Similarity to Olivia : 0.037374873\n",
      "Ethnicity: African | Similarity to Olivia : -0.03661616 \n",
      "\n",
      "Region: Americas | Similarity to Olivia : 0.11779842\n",
      "Ethnicity: American | Similarity to Olivia : -0.03126251 \n",
      "\n",
      "Region: Asia | Similarity to Olivia : 0.012866648\n",
      "Ethnicity: Asian | Similarity to Olivia : -0.048824266 \n",
      "\n",
      "Region: Europe | Similarity to Olivia : -0.006574763\n",
      "Ethnicity: European | Similarity to Olivia : -0.02915873 \n",
      "\n",
      "Region: Oceania | Similarity to Olivia : 0.15092842\n",
      "Ethnicity: Oceanic | Similarity to Olivia : 0.14548078 \n",
      "\n",
      "Most similiar region to Olivia : Oceania\n",
      "Most similiar ethnicity to Olivia : Oceanic\n"
     ]
    }
   ],
   "source": [
    "region_ethnicity_name(\"Olivia\",regions,ethnicities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65648dc7",
   "metadata": {},
   "source": [
    "**5.1 Answer this:** Visit the wikipedia article. Does Olivia appear as a common name for Oceania? Could you propose a reason why this happened? Or even more so, do you believe there is a real reason that we could hope to rationalize?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c3098",
   "metadata": {},
   "source": [
    "**5.2 Code this:** Try another name, perhaps your own! If you're feeling ambitious, maybe change the regions and associated ethnicities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d44a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE THIS\n",
    "#region_ethnicity_name(\"...\",regions,ethnicities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f20d8",
   "metadata": {},
   "source": [
    "While the paper we are about to mention is far beyond the scope of this assignment, we wanted to relate something we all share - a name with something more scientific. No matter who you are, you probably know someone who has or you yourself have been adversely affected by your name. This paper https://www.pnas.org/content/115/16/E3635 talks about many forms of bias including stereotypes, but one section is dedicated to \"Top Asian (vs White) Adjectives over time by relative norm difference\". They were concerned about the differences over the course of different decades. What we want to highlight is the infinite amount of dimensions we have yet to think about when it comes to bias in word embeddings. While we use a specific corpus trained using a specific implementation of Word2Vec by gensim, we may or may not see certain forms of bias. \n",
    "\n",
    "Below, we offer just one small glimpse at what we mean. Above we briefly motivated how machines just like we do (implicitly or explcitly) associate certain names with regions, ethnicities, race, gender, etc. In turn, we also associate all of these including name with certain steoreotypes or associations. So what if we take a list of adjectives which the paper cites on page 32 here: https://www.pnas.org/content/pnas/suppl/2018/03/30/1720347115.DCSupplemental/pnas.1720347115.sapp.pdf and test out two names, one traditionally Asian and one typically white?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "850316df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inhibited\n",
      "-0.030865418\n",
      "-0.0025761127\n",
      "Lee is more related to being inhibited\n",
      "\n",
      "\n",
      "passive\n",
      "0.04255561\n",
      "0.057316415\n",
      "Lee is more related to being passive\n",
      "\n",
      "\n",
      "dissolute\n",
      "0.030017145\n",
      "0.053788844\n",
      "Lee is more related to being dissolute\n",
      "\n",
      "\n",
      "haughty\n",
      "-0.022266619\n",
      "-0.008647712\n",
      "Lee is more related to being haughty\n",
      "\n",
      "\n",
      "complacent\n",
      "0.1176846\n",
      "0.0627808\n",
      "Johnson is more related to being complacent\n",
      "\n",
      "\n",
      "forceful\n",
      "0.108973734\n",
      "0.0932092\n",
      "Johnson is more related to being forceful\n",
      "\n",
      "\n",
      "fixed\n",
      "0.029848136\n",
      "-0.01067044\n",
      "Johnson is more related to being fixed\n",
      "\n",
      "\n",
      "active\n",
      "0.034025878\n",
      "-0.00011331774\n",
      "Johnson is more related to being active\n",
      "\n",
      "\n",
      "sensitive\n",
      "-0.041375607\n",
      "0.024026982\n",
      "Lee is more related to being sensitive\n",
      "\n",
      "\n",
      "hearty\n",
      "0.0135062765\n",
      "0.05642234\n",
      "Lee is more related to being hearty\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjectives = [\"inhibited\",\"passive\",\"dissolute\",\"haughty\",\"complacent\",\"forceful\",\"fixed\",\"active\",\"sensitive\",\"hearty\"]\n",
    "names = [\"Johnson\",\"Lee\"]\n",
    "for adjective in adjectives:\n",
    "    print(adjective)\n",
    "    print(model.similarity(adjective,names[0]))\n",
    "    print(model.similarity(adjective,names[1]))\n",
    "    if model.similarity(adjective,names[0]) > model.similarity(adjective,names[1]):\n",
    "        print(names[0],\"is more related to being\",adjective)\n",
    "    else:\n",
    "        print(names[1],\"is more related to being\",adjective)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec009d",
   "metadata": {},
   "source": [
    "**5.3 Answer this:** What do you notice? Can you think of an algorithm, system, etc. that makes use of a person's name and how word embeddings might adversely affect someone?\n",
    "\n",
    "*Your answer:* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf81e9c",
   "metadata": {},
   "source": [
    "**5.4 Code this:** Visit page 32 of the appendix. Here it is linked again: https://www.pnas.org/content/pnas/suppl/2018/03/30/1720347115.DCSupplemental/pnas.1720347115.sapp.pdf\n",
    "Notice we used the set of words from 1910. Try a different decade (perhaps a more recent one?) and potentially two other names. Try out the same activity or if you are more comfortable in python something that interests you more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f1455811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE THIS\n",
    "adjectives = [...]\n",
    "names = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e8191",
   "metadata": {},
   "source": [
    "# Section 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e507733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlantic\n",
      "0.06400934\n",
      "0.085823916\n",
      "\n",
      "\n",
      "Bergen\n",
      "0.12048365\n",
      "0.13565809\n",
      "\n",
      "\n",
      "Burlington\n",
      "0.0971214\n",
      "0.11556664\n",
      "\n",
      "\n",
      "Camden\n",
      "0.12468157\n",
      "0.12762818\n",
      "\n",
      "\n",
      "Cumberland\n",
      "0.10954588\n",
      "0.10174918\n",
      "\n",
      "\n",
      "Essex\n",
      "0.104827315\n",
      "0.10680808\n",
      "\n",
      "\n",
      "Gloucester\n",
      "0.14600462\n",
      "0.15054289\n",
      "\n",
      "\n",
      "Hudson\n",
      "0.12544172\n",
      "0.08396277\n",
      "\n",
      "\n",
      "Hunterdon\n",
      "0.13188195\n",
      "0.17527124\n",
      "\n",
      "\n",
      "Mercer\n",
      "0.11866079\n",
      "0.052931145\n",
      "\n",
      "\n",
      "Middlesex\n",
      "0.13007653\n",
      "0.17929542\n",
      "\n",
      "\n",
      "Monmouth\n",
      "0.17148839\n",
      "0.16626918\n",
      "\n",
      "\n",
      "Morris\n",
      "0.113886386\n",
      "0.08832838\n",
      "\n",
      "\n",
      "Ocean\n",
      "0.081826314\n",
      "0.1341419\n",
      "\n",
      "\n",
      "Passaic\n",
      "0.13193662\n",
      "0.15426777\n",
      "\n",
      "\n",
      "Salem\n",
      "0.09963668\n",
      "0.09049606\n",
      "\n",
      "\n",
      "Somerset\n",
      "0.089178614\n",
      "0.09955392\n",
      "\n",
      "\n",
      "Sussex\n",
      "0.06282275\n",
      "0.043510772\n",
      "\n",
      "\n",
      "Union\n",
      "0.096256495\n",
      "0.18805102\n",
      "\n",
      "\n",
      "Warren\n",
      "0.12295346\n",
      "0.1081308\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NJ_counties = [\"Atlantic\",\"Bergen\",\"Burlington\",\"Camden\",\"Cumberland\",\"Essex\",\"Gloucester\",\"Hudson\",\"Hunterdon\",\"Mercer\",\"Middlesex\",\"Monmouth\",\"Morris\",\"Ocean\",\"Passaic\",\"Salem\",\"Somerset\",\"Sussex\",\"Union\",\"Warren\"]\n",
    "parties = [\"Republican\",\"Democratic\"]\n",
    "for county in NJ_counties:\n",
    "    print(county)\n",
    "    print(model.similarity(county,parties[0]))\n",
    "    print(model.similarity(county,parties[1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "132e7641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Republican', 0.8078877925872803),\n",
       " ('Democratic', 0.7795360088348389),\n",
       " ('Demo_crat', 0.6948478817939758),\n",
       " ('Democrats', 0.6871914267539978),\n",
       " ('GOP', 0.672263503074646),\n",
       " ('senator', 0.6551626324653625),\n",
       " ('congressman', 0.6531549692153931),\n",
       " ('Sen.', 0.651481568813324),\n",
       " ('Republicans', 0.638499915599823),\n",
       " ('democrat', 0.635062575340271)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Democrat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9aa93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
